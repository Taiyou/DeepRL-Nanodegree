{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "In the project, two agents play tennis although I think this is rather badminton. The goal of the task is to keep hitting a ball collaboratively with two agents. This is a continuous control task, so I used the deep determinitic gradient policy (DDPG)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Algorithm and parameter setting\n",
    "## 2.1 Problem setting\n",
    "2 agents are required to track balls. The agent has 8 space size orresponding to variables corresponding to the position and velocity of the ball and racket, and action space is 2.\n",
    "### Unity Academy name: Academy\n",
    "       - Number of Brains: 1\n",
    "       - Number of External Brains : 1\n",
    "       - Lesson number : 0\n",
    "       - Reset Parameters :\n",
    "\t\t\n",
    "### Unity brain name: TennisBrain\n",
    "       - Number of Visual Observations (per agent): 0\n",
    "       - Vector Observation space type: continuous\n",
    "       - Vector Observation space size (per agent): 8\n",
    "       - Number of stacked Vector Observation: 3\n",
    "       - Vector Action space type: continuous\n",
    "       - Vector Action space size (per agent): 2\n",
    "       - Vector Action descriptions: , \n",
    "       \n",
    "       \n",
    "## 2.2 \n",
    "The deep is composed of a deep neural network with the actor critic method and the deep deterministic policy gradient.\n",
    "\n",
    "For the actor\n",
    "- first layer, size: 33, fully connected with the second layer through the relu funciton\n",
    "- second layer, size: 256, fully connected with the third layer through the relu funciton and batch normalization\n",
    "- third layer, size: 128, fully connected with the output layer through the the hyperbolic tangent function\n",
    "- output layer, output size: 4 (action size),\n",
    "\n",
    "\n",
    "For the critic\n",
    "- first layer, input size: 33, fully connected with the second layer through the relu funciton\n",
    "- second layer, input size: 256, fully connected with the third layer through the relu funciton and batch normalization\n",
    "- third layer, input size: 128 + 4 action size , fully connected with the output layer through the the hyperbolic tangent function\n",
    "- output layer, input size: 1\n",
    "\n",
    "## 2.3 Hyperparameters\n",
    "Parameter used in the algorithm is\n",
    "\n",
    "- replay buffer size = 1e5\n",
    "- minibatch size = 128\n",
    "- discount factor gamma = 0.99\n",
    "- soft update tau = 1e-3\n",
    "- learning rate for the actor = 2e-4\n",
    "- learning rate for the critic = 2e-4 Ornstein-Uhlenbeck noise parameters\n",
    "- theta: 0.15\n",
    "- sigma: 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results\n",
    "Solving criterion is set to average score over 0.5. My result showed that the algorithm solved within 655 episodes.\n",
    "\n",
    "- Episode 0 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 10 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 20 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 30 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 40 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 50 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 60 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 70 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.00\n",
    "- Episode 80 \t Score: 0.00 \tSimple average Score: -0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 90 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.011\n",
    "- Episode 100 \t Score: 0.09 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 110 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 120 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.02\n",
    "- Episode 130 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 140 \t Score: 0.10 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 150 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 160 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 170 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 180 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 190 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 200 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 210 \t Score: 0.10 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 220 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 230 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 240 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 250 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.02\n",
    "- Episode 260 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 270 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 280 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 290 \t Score: 0.10 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 300 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 310 \t Score: 0.10 \tSimple average Score: -0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 320 \t Score: 0.09 \tSimple average Score: -0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 330 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.011\n",
    "- Episode 340 \t Score: 0.10 \tSimple average Score: 0.00, \tAverage maximum Score: 0.01\n",
    "- Episode 350 \t Score: 0.00 \tSimple average Score: 0.00, \tAverage maximum Score: 0.02\n",
    "- Episode 360 \t Score: 0.10 \tSimple average Score: 0.01, \tAverage maximum Score: 0.02\n",
    "- Episode 370 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.03\n",
    "- Episode 380 \t Score: 0.00 \tSimple average Score: 0.01, \tAverage maximum Score: 0.03\n",
    "- Episode 390 \t Score: 0.10 \tSimple average Score: 0.02, \tAverage maximum Score: 0.04\n",
    "- Episode 400 \t Score: 0.10 \tSimple average Score: 0.02, \tAverage maximum Score: 0.05\n",
    "- Episode 410 \t Score: 0.10 \tSimple average Score: 0.03, \tAverage maximum Score: 0.06\n",
    "- Episode 420 \t Score: 0.10 \tSimple average Score: 0.03, \tAverage maximum Score: 0.06\n",
    "- Episode 430 \t Score: 0.10 \tSimple average Score: 0.03, \tAverage maximum Score: 0.07\n",
    "- Episode 440 \t Score: 0.10 \tSimple average Score: 0.04, \tAverage maximum Score: 0.08\n",
    "- Episode 450 \t Score: 0.10 \tSimple average Score: 0.05, \tAverage maximum Score: 0.09\n",
    "- Episode 460 \t Score: 0.09 \tSimple average Score: 0.05, \tAverage maximum Score: 0.09\n",
    "- Episode 470 \t Score: 0.10 \tSimple average Score: 0.05, \tAverage maximum Score: 0.09\n",
    "- Episode 480 \t Score: 0.10 \tSimple average Score: 0.06, \tAverage maximum Score: 0.10\n",
    "- Episode 490 \t Score: 0.50 \tSimple average Score: 0.08, \tAverage maximum Score: 0.12\n",
    "- Episode 500 \t Score: 0.30 \tSimple average Score: 0.18, \tAverage maximum Score: 0.22\n",
    "- Episode 510 \t Score: 0.60 \tSimple average Score: 0.22, \tAverage maximum Score: 0.26\n",
    "- Episode 520 \t Score: 2.10 \tSimple average Score: 0.28, \tAverage maximum Score: 0.32\n",
    "- Episode 530 \t Score: 1.90 \tSimple average Score: 0.33, \tAverage maximum Score: 0.36\n",
    "- Episode 540 \t Score: 0.20 \tSimple average Score: 0.36, \tAverage maximum Score: 0.39\n",
    "- Episode 550 \t Score: 0.80 \tSimple average Score: 0.37, \tAverage maximum Score: 0.41\n",
    "- Episode 560 \t Score: 0.10 \tSimple average Score: 0.37, \tAverage maximum Score: 0.41\n",
    "- Episode 570 \t Score: 0.70 \tSimple average Score: 0.39, \tAverage maximum Score: 0.43\n",
    "- Episode 580 \t Score: 0.40 \tSimple average Score: 0.40, \tAverage maximum Score: 0.44\n",
    "- Episode 590 \t Score: 0.10 \tSimple average Score: 0.40, \tAverage maximum Score: 0.44\n",
    "- Episode 600 \t Score: 0.30 \tSimple average Score: 0.32, \tAverage maximum Score: 0.35\n",
    "- Episode 610 \t Score: 0.90 \tSimple average Score: 0.31, \tAverage maximum Score: 0.34\n",
    "- Episode 620 \t Score: 0.69 \tSimple average Score: 0.28, \tAverage maximum Score: 0.32\n",
    "- Episode 630 \t Score: 0.60 \tSimple average Score: 0.28, \tAverage maximum Score: 0.32\n",
    "- Episode 640 \t Score: 2.70 \tSimple average Score: 0.33, \tAverage maximum Score: 0.37\n",
    "- Episode 650 \t Score: 0.00 \tSimple average Score: 0.41, \tAverage maximum Score: 0.44\n",
    "- Episode 655 \t Score: 2.60 \tSimple average Score: 0.49, \tAverage maximum Score: 0.52\n",
    "\n",
    "![results](results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Future suggestions\n",
    "For the project, I used DDPG. The other advanced methods are nice to try. Especially, MADDPG is specialized for multiple agents, so average scores of the two agents must be increased quickly. \n",
    "- [Asynchronous Actor-Critic Agents (A3C)](https://medium.com/emergent-future/simple-reinforcement-learning-with-tensorflow-part-8-asynchronous-actor-critic-agents-a3c-c88f72a5e9f2)\n",
    "- [Trust Region Policy Optimization (TRPO) and Proximal Policy Optimization (PPO)](https://medium.com/@sanketgujar95/trust-region-policy-optimization-trpo-and-proximal-policy-optimization-ppo-e6e7075f39ed)\n",
    "- [Multi-agent deep deterministic policy gradient (MADDPG) approach](https://papers.nips.cc/paper/7217-multi-agent-actor-critic-for-mixed-cooperative-competitive-environments.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
