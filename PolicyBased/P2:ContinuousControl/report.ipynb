{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "This is a report for the second project in deep reinforcement learning nanodegree. In the project, 20 agent arms are required to track balls which move individually with different speed. The task is a continuous control task, so [the deep deterministic policy gradient (DDPG)](https://arxiv.org/abs/1509.02971) approach was employed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Problem setting and Algorithm\n",
    "## 2.1 Problem setting\n",
    "20 agents are required to track balls. The agent has 33 space size orresponding to position, rotation, velocity, and angular velocities of the arm. , and action space is 4 (move forward, move backward, turn left, and turn right).\n",
    "\n",
    "### Unity Academy name: Academy\n",
    "        -  Number of Brains: 1\n",
    "        - Number of External Brains : 1\n",
    "        - Lesson number : 0\n",
    "        - Reset Parameters :\n",
    "\t\t  - goal_speed -> 1.0\n",
    "\t\t  - goal_size -> 5.0\n",
    "### Unity brain name: ReacherBrain\n",
    "        - Number of Visual Observations (per agent): 0\n",
    "        - Vector Observation space type: continuous\n",
    "        - Vector Observation space size (per agent): 33\n",
    "        - Number of stacked Vector Observation: 1\n",
    "        - Vector Action space type: continuous\n",
    "        - Vector Action space size (per agent): 4\n",
    "        - Vector Action descriptions: , , , \n",
    "        \n",
    " ## 2.2 Algorithm\n",
    "The deep is composed of a deep neural network with the actor critic method and the deep deterministic policy gradient. \n",
    "\n",
    "For the actor\n",
    "- first layer, size: 33, fully connected with the second layer through the relu funciton\n",
    "- second layer, size: 256, fully connected with the third layer through the relu funciton and batch normalization\n",
    "- third layer, size: 128, fully connected with the output layer through the the hyperbolic tangent function\n",
    "- output layer, output size: 4 (action size), \n",
    "\n",
    "For the critic\n",
    "- first layer, input size: 33, fully connected with the second layer through the relu funciton\n",
    "- second layer, input size: 256, fully connected with the third layer through the relu funciton and batch normalization\n",
    "- third layer, input size:  128 + 4 action size , fully connected with the output layer through the the hyperbolic tangent function\n",
    "- output layer, input size: 1\n",
    "\n",
    "## 2.3 Hyperparameters\n",
    "Parameter used in the algorithm is\n",
    "- replay buffer size = 1e5 \n",
    "- minibatch size  = 128 \n",
    "- discount factor gamma = 0.99\n",
    "- soft update tau = 1e-3\n",
    "- learning rate for the actor = 2e-4\n",
    "- learning rate for the critic = 2e-4\n",
    "Ornstein-Uhlenbeck noise parameters\n",
    "- theta: 0.15\n",
    "- sigma: 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Results\n",
    "Solving criterion is set to average score over 30. My result showed that the algorithm solved within 52 episodes.\n",
    "- Episode 0 \t Score: 0.90 \tAverage Score: 0.90\n",
    "- Episode 10 \t Score: 32.43 \tAverage Score: 15.25\n",
    "- Episode 20 \t Score: 34.49 \tAverage Score: 22.92\n",
    "- Episode 30 \t Score: 35.22 \tAverage Score: 26.77\n",
    "- Episode 40 \t Score: 36.85 \tAverage Score: 28.46\n",
    "- Episode 50 \t Score: 35.54 \tAverage Score: 29.82\n",
    "- Episode 52 \t Score: 34.07 \tAverage Score: 30.02\n",
    "- Environment solved in 52 episodes!\tAverage Score: 30.02\n",
    "\n",
    "![Results of averageScores](AverageScores.png)\n",
    "\n",
    "This is how the trained agents behave.\n",
    "\n",
    "![Trained Agents](giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Future suggestions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm learn with a small number of episodes, but learning time was very expensive. Therefore, it is worth to try the prioritized experience relay with DDPG for the faster learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
