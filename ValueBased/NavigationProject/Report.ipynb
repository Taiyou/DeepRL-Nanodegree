{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "This is a report for the first project in deep reinforcement learning. In the project, an agent is to maximize average return by obtaining yellow bananas while minimizing negative returns by avoiding blue bananas. I used [the original deep Q-network(Mnih et al., 2015)](https://www.ncbi.nlm.nih.gov/pubmed/25719670) with soft update and replay buffer for this project.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem setting and Algorithm\n",
    "## Problem setting\n",
    "An agent is require to explore an open space to pick up yellow bananas while avoiding blue bananas. The agent has 37 space size, and action space is 4 (move forward, move backward, turn left, and turn right).\n",
    "\n",
    "- Unity Academy name: Academy\n",
    "        Number of Brains: 1\n",
    "        Number of External Brains : 1\n",
    "        Lesson number : 0\n",
    "        Reset Parameters :\n",
    "\t\t\n",
    "- Unity brain name: BananaBrain\n",
    "        Number of Visual Observations (per agent): 0\n",
    "        Vector Observation space type: continuous\n",
    "        Vector Observation space size (per agent): 37\n",
    "        Number of stacked Vector Observation: 1\n",
    "        Vector Action space type: discrete\n",
    "        Vector Action space size (per agent): 4\n",
    "        Vector Action descriptions: , , , \n",
    "\n",
    "## Algorithm\n",
    "The deep Q-network is composed of a deep neural network. \n",
    "\n",
    "In this experiment, we have 4 layers and each layer is fully connected through a nonlinear relu function.\n",
    "- 1st layer, input: state_size(n=37), output: 64\n",
    "- 2nd layer, input: 64, output: 64\n",
    "- 3rd layer, input: 64, output: 32\n",
    "- 4th layer, input: 32, output: action_size(n=4)\n",
    "\n",
    "### Parameter setting\n",
    "Parameters used in DQN algorithm:\n",
    "\n",
    "- Maximum steps per episode: 1000\n",
    "- Starting epsilion: 1.0\n",
    "- Ending epsilion: 0.01\n",
    "- Epsilion decay rate: 0.995"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results\n",
    "Solving criterion is set to average score 16. My result showed that the algorithm solved within 900 episodes.\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "- Episode 100\tAverage Score: 1.31\n",
    "- Episode 200\tAverage Score: 4.54\n",
    "- Episode 300\tAverage Score: 9.08\n",
    "- Episode 400\tAverage Score: 10.71\n",
    "- Episode 500\tAverage Score: 12.44\n",
    "- Episode 600\tAverage Score: 14.30\n",
    "- Episode 700\tAverage Score: 14.78\n",
    "- Episode 800\tAverage Score: 15.48\n",
    "- Episode 900\tAverage Score: 15.60\n",
    "- Episode 975\tAverage Score: 16.07\n",
    "- Environment solved in 875 episodes!\tAverage Score: 16.07\n",
    "\n",
    "---------------------------------------------------------------------------\n",
    "\n",
    "![Results](download.png)\n",
    "\n",
    "This is how the trained agent behave.\n",
    "![Trained Subjects](TrainedDQN.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future suggestions\n",
    "In this project, our result with deep Q-networks which was originally published, successfully showed improvement of agents. For further improvements, \n",
    "1. [prioritized experience reply(Schaul et al., 2015)](https://arxiv.org/abs/1511.05952)\n",
    "2. [dueling DQN(Hasselt et al., 2015)](http://proceedings.mlr.press/v48/wangf16.pdf),\n",
    "3. [double DQN(Wang et al., 2015)](https://arxiv.org/pdf/1509.06461.pdf)\n",
    "\n",
    "should be implemented to compare their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "drlnd",
   "language": "python",
   "name": "drlnd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
